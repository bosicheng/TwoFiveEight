{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import urllib.request\n",
    "import scipy.optimize\n",
    "import random\n",
    "from collections import defaultdict\n",
    "import nltk\n",
    "import string\n",
    "from nltk.stem.porter import *\n",
    "from sklearn import linear_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parseData(fname):\n",
    "    for l in urllib.request.urlopen(fname):\n",
    "        yield eval(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading data......\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "print (\"Reading data......\")\n",
    "data = list(parseData(\"http://jmcauley.ucsd.edu/cse190/data/beer/beer_50000.json\"))[:5000]\n",
    "print (\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'review/appearance': 3.0, 'beer/style': 'English Strong Ale', 'review/palate': 3.0, 'review/taste': 3.0, 'beer/name': 'Red Moon', 'review/timeUnix': 1235915097, 'beer/ABV': 6.2, 'beer/beerId': '48213', 'beer/brewerId': '10325', 'review/timeStruct': {'isdst': 0, 'mday': 1, 'hour': 13, 'min': 44, 'sec': 57, 'mon': 3, 'year': 2009, 'yday': 60, 'wday': 6}, 'review/overall': 3.0, 'review/text': 'Dark red color, light beige foam, average.\\tIn the smell malt and caramel, not really light.\\tAgain malt and caramel in the taste, not bad in the end.\\tMaybe a note of honey in teh back, and a light fruitiness.\\tAverage body.\\tIn the aftertaste a light bitterness, with the malt and red fruit.\\tNothing exceptional, but not bad, drinkable beer.', 'user/profileName': 'stcules', 'review/aroma': 2.5}\n"
     ]
    }
   ],
   "source": [
    "print (data[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordCount = defaultdict(int)\n",
    "punctuation = set(string.punctuation)\n",
    "for d in data:\n",
    "    r = ''.join([c for c in d['review/text'].lower() if not c in punctuation])\n",
    "    for w in r.split():\n",
    "        wordCount[w] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All together there are 19426 words\n"
     ]
    }
   ],
   "source": [
    "print (f\"All together there are {len(wordCount)} words\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts_w = [(wordCount[w], w) for w in wordCount]\n",
    "counts_w.sort()\n",
    "counts_w.reverse()\n",
    "words = [x[1] for x in counts_w[:1000]]\n",
    "wordId = dict(zip(words, range(len(words))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_list = defaultdict(list)\n",
    "for i in range(len(data)):\n",
    "    r = ''.join([c for c in data[i]['review/text'].lower() if not c in punctuation])\n",
    "    text_list[i] = r.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_list_with_punc = defaultdict(list)\n",
    "for i in range(len(data)):\n",
    "    r = ''.join([c if not c in punctuation else ' '+c+' ' for c in data[i]['review/text'].lower()])\n",
    "    text_list_with_punc[i] = r.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'lot', 'of', 'foam', 'but', 'a', 'lot', 'in', 'the', 'smell', 'some', 'banana', 'and', 'then', 'lactic', 'and', 'tart', 'not', 'a', 'good', 'start', 'quite', 'dark', 'orange', 'in', 'color', 'with', 'a', 'lively', 'carbonation', 'now', 'visible', 'under', 'the', 'foam', 'again', 'tending', 'to', 'lactic', 'sourness', 'same', 'for', 'the', 'taste', 'with', 'some', 'yeast', 'and', 'banana']\n"
     ]
    }
   ],
   "source": [
    "print (text_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'lot', 'of', 'foam', '.', 'but', 'a', 'lot', '.', 'in', 'the', 'smell', 'some', 'banana', ',', 'and', 'then', 'lactic', 'and', 'tart', '.', 'not', 'a', 'good', 'start', '.', 'quite', 'dark', 'orange', 'in', 'color', ',', 'with', 'a', 'lively', 'carbonation', '(', 'now', 'visible', ',', 'under', 'the', 'foam', ')', '.', 'again', 'tending', 'to', 'lactic', 'sourness', '.', 'same', 'for', 'the', 'taste', '.', 'with', 'some', 'yeast', 'and', 'banana', '.']\n"
     ]
    }
   ],
   "source": [
    "print (text_list_with_punc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the freq of word in all documents\n",
    "each_word_freq_doc = defaultdict(int)\n",
    "for each_word in wordCount:\n",
    "    freq = 0\n",
    "    for i in range(len(text_list)):\n",
    "        if each_word in text_list[i]:\n",
    "            freq += 1\n",
    "    each_word_freq_doc[each_word] = freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "Bigram = defaultdict(int)\n",
    "punctuation = set(string.punctuation)\n",
    "for d in data:\n",
    "    r = ''.join([c for c in d['review/text'].lower() if not c in punctuation])\n",
    "    for i in range(len(r.split())-1):\n",
    "        Bigram[r.split()[i]+\"-\"+r.split()[i+1]] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4587, 'with-a')\n"
     ]
    }
   ],
   "source": [
    "print (max(zip(Bigram.values(),Bigram.keys())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4587\n"
     ]
    }
   ],
   "source": [
    "print(Bigram['with-a'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "print (Bigram['deal-with'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = [(Bigram[biw], biw) for biw in Bigram]\n",
    "counts.sort()\n",
    "counts.reverse()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "bi_words = [x[1] for x in counts[:1000]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentiment analysis\n",
    "bi_wordID = dict(zip(bi_words, range(len(bi_words))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print (bi_wordID)\n",
    "bi_wordSet = set(bi_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature(datum):\n",
    "    feat = [0]*len(bi_words)\n",
    "    r = ''.join([c for c in datum['review/text'].lower() if not c in punctuation])\n",
    "    for i in range(len(r.split())-1):\n",
    "        bi_unit = r.split()[i]+\"-\"+r.split()[i+1]\n",
    "        if  bi_unit in bi_words:\n",
    "            feat[bi_wordID[bi_unit]] += 1\n",
    "    feat.append(1)\n",
    "    return feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = [feature(d) for d in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = [d['review/overall'] for d in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = linear_model.Ridge(1.0, fit_intercept=False)\n",
    "clf.fit(X,Y)\n",
    "theta = clf.coef_\n",
    "predictions = clf.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3.48471909 3.31957086 3.54264439 ... 5.20157626 3.53660705 4.27659128]\n"
     ]
    }
   ],
   "source": [
    "print (predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "# report the MSE on the 5000 data\n",
    "MSE = sum([(predictions[i]-Y[i])**2 for i in range(len(Y))])/len(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE obtained using the new predictor: 0.3431530140613639\n"
     ]
    }
   ],
   "source": [
    "print (f\"MSE obtained using the new predictor: {MSE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'int'>, {'foam': 364, 'smell': 1449, 'banana': 105, 'lactic': 6, 'tart': 78})\n"
     ]
    }
   ],
   "source": [
    "# total number of documents is N = 5000\n",
    "# Compute IDF for ‘foam’, ‘smell’, ‘banana’, ‘lactic’, and ‘tart’\n",
    "goal_word = ['foam', 'smell', 'banana', 'lactic', 'tart']\n",
    "freq = defaultdict(int)\n",
    "for d in data:\n",
    "    r = ''.join([c for c in d['review/text'].lower() if not c in punctuation])\n",
    "    for elem in goal_word:\n",
    "        if elem in r.split():\n",
    "            freq[elem] += 1\n",
    "print (freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "IDF = [np.log10(5000/freq[elem]) for elem in freq]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.1378686206869628, 0.5379016188648442, 1.6777807052660807, 2.9208187539523753, 1.8068754016455384]\n"
     ]
    }
   ],
   "source": [
    "print (IDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 1, 2, 2, 1]\n"
     ]
    }
   ],
   "source": [
    "# tf: number of times the term appears in the document\n",
    "first_review = data[0]['review/text']\n",
    "r = ''.join([c for c in first_review.lower() if not c in punctuation])\n",
    "tf = [0]*len(goal_word)\n",
    "for i in range(len(goal_word)):\n",
    "    for elem in r.split():\n",
    "        if elem == goal_word[i]:\n",
    "            tf[i] += 1\n",
    "print (tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute TF-IDF\n",
    "TF_IDF = [tf[i]*IDF[i] for i in range(len(tf))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.2757372413739256, 0.5379016188648442, 3.3555614105321614, 5.841637507904751, 1.8068754016455384]\n"
     ]
    }
   ],
   "source": [
    "print (TF_IDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ComputeCosineSImilarity(x, y):\n",
    "    res = 0\n",
    "    for elem_x in x:\n",
    "        if elem_x in y:\n",
    "            res = res + x[elem_x]*y[elem_x]\n",
    "    part1 = (sum([x[elem]**2 for elem in x]))**(1/2)\n",
    "    part2 = (sum([y[elem]**2 for elem in y]))**(1/2)\n",
    "    return res/(part1*part2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeTF_IDF(gword, text_list, each_word_freq_doc, index):\n",
    "    unit_freq = each_word_freq_doc[gword]\n",
    "    tf = 0\n",
    "    for elem in text_list[index]:\n",
    "        if elem == gword:\n",
    "            tf += 1\n",
    "    return tf*(np.log10(5000/(1+unit_freq)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build tf-idf vector\n",
    "def tf_idf_builder(index, text_list, ):\n",
    "    review_w = text_list[index]\n",
    "    rev_vec = defaultdict(float)\n",
    "    for elem in review_w:\n",
    "        if elem not in rev_vec:\n",
    "            rev_vec[elem] = computeTF_IDF(elem, text_list, each_word_freq_doc, index)\n",
    "    return rev_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'float'>, {'a': 0.02414000721952438, 'lot': 2.022882086242769, 'of': 0.05148923116234282, 'foam': 2.273354279759088, 'but': 0.1662156253435211, 'in': 0.3494074711380801, 'the': 0.08645087743151567, 'smell': 0.5376020021010439, 'some': 0.6729750591696887, 'banana': 3.347328278142497, 'and': 0.09763689545177756, 'then': 1.0034883278458213, 'lactic': 5.707743928643524, 'tart': 1.8013429130455774, 'not': 0.28216313251307434, 'good': 0.3701828039814841, 'start': 1.4975728800155672, 'quite': 0.8096683018297085, 'dark': 0.5509846836522136, 'orange': 0.7894139750948435, 'color': 0.46042211665469096, 'with': 0.12416219470443926, 'lively': 1.9586073148417749, 'carbonation': 0.36916548217194944, 'now': 1.4023048140744876, 'visible': 1.9136401693252518, 'under': 1.7544873321858503, 'again': 0.8781120148963188, 'tending': 2.9208187539523753, 'to': 0.1305335899191335, 'sourness': 2.0861861476162833, 'same': 1.2856702402547668, 'for': 0.288192770958809, 'taste': 0.2912392763096833, 'yeast': 1.3027706572402824})\n"
     ]
    }
   ],
   "source": [
    "# construct tf-idf vector for review1 and review2\n",
    "review1w =  text_list[0]\n",
    "review2w =  text_list[1]\n",
    "rev_1_vec = tf_idf_builder(0, text_list)\n",
    "rev_2_vec = tf_idf_builder(1, text_list)\n",
    "print (rev_1_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine similarity between review1 and review2 is 0.06691778465356775\n"
     ]
    }
   ],
   "source": [
    "print (f\"Cosine similarity between review1 and review2 is {ComputeCosineSImilarity(rev_1_vec, rev_2_vec)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.024680501910290382\n",
      "0.0389329943130906\n",
      "0.011928781805429233\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:8: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.007144706263166458\n"
     ]
    }
   ],
   "source": [
    "# beer name\n",
    "# text_of_review\n",
    "# profile_name\n",
    "max_similarity = -1.0\n",
    "beer_name = data[0]['beer/name']\n",
    "text_of_review = data[0]['review/text']\n",
    "profile_name = data[0]['user/profileName']\n",
    "for i in range(1, len(data)):\n",
    "    new_vec = tf_idf_builder(i, text_list)\n",
    "    simi =  ComputeCosineSImilarity(rev_1_vec, new_vec)\n",
    "    if simi > max_similarity:\n",
    "        max_similarity = simi\n",
    "        beer_name = data[i]['beer/name']\n",
    "        text_of_review = data[i]['review/text']\n",
    "        profile_name = data[i]['user/profileName']\n",
    "    if i % 1000 == 0:\n",
    "        print (simi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Her Majesty 2011\n",
      "750mL bottle thanks to Chris@Slowbeer. Poured into a Lost Abbey stemmed tulip.\t\tGolden orange, close to translucent (on the first pour at least), capped by a sizable white, typically Belgian-looking head. Good lacing.\t\tQuite strong lactic notes and a sharp organic funk. Pungent stuff. Underneath is bitter citrus pith, floral spice and a hint of sweet esters. In your face with a lot going on. Only issue is the lactic character verges on turning my stomach.\t\tMore citric sourness and a bit less lactic character. Grapefruit and lemon rind are prominent, as is the Nelson Sauvin vegetative character, which kind of adheres to the yeast and barnyard funk. Tropical melons and honey provide some sweetness. Decent peppery tang.\t\tMedium, lightly syrupy body with lowish carbonation and a moderately tart, dry finish that has some length to it.\t\tIncomparable to anything I've tried. The Sauvin hops with the Saison yeast is a masterful combination, however there's no shortage of rough edges, which prevents an amazing or highly drinkable result.\n",
      "spicelab\n",
      "0.2965167049117193\n"
     ]
    }
   ],
   "source": [
    "# output the goal_beer\n",
    "print (beer_name)\n",
    "print (text_of_review)\n",
    "print (profile_name)\n",
    "print (max_similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_feature(index, text_list, each_word_freq_doc):\n",
    "    feat = [0]*len(words)\n",
    "    goal_review = text_list[index]\n",
    "    for w in goal_review:\n",
    "        if w in words and feat[wordId[w]] == 0:\n",
    "            feat[wordId[w]] = computeTF_IDF(w, text_list, each_word_freq_doc, index)\n",
    "    feat.append(1)\n",
    "    return feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = [new_feature(index, text_list, each_word_freq_doc) for index in range(len(text_list))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = linear_model.Ridge(1.0, fit_intercept=False)\n",
    "clf.fit(X,Y)\n",
    "theta = clf.coef_\n",
    "predictions = clf.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3.09655868 3.57328571 3.58483271 ... 4.290122   3.42816778 4.24918487]\n"
     ]
    }
   ],
   "source": [
    "print (predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "MSE_TF_IDF = sum([(predictions[i]-Y[i])**2 for i in range(len(Y))])/len(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE of predict model base on tfidf feature is 0.27875971411652656\n"
     ]
    }
   ],
   "source": [
    "print (f\"MSE of predict model base on tfidf feature is {MSE_TF_IDF}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading data......\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "# for question 7\n",
    "# first we shuffle the data\n",
    "print (\"Reading data......\")\n",
    "data_all = list(parseData(\"http://jmcauley.ucsd.edu/cse190/data/beer/beer_50000.json\"))\n",
    "print (\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000\n"
     ]
    }
   ],
   "source": [
    "random.shuffle(data_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data into training set, validation set and testing set\n",
    "data_train = data_all[:5000]\n",
    "data_validate = data_all[5000+1:5001+5000]\n",
    "data_test = data_all[5001+5000+1:5001+5000+1+5000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we store data in memory-unigram\n",
    "# remove punctuation or not remove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_list_m = defaultdict(list)\n",
    "for i in range(len(data_train)):\n",
    "    r = ''.join([c for c in data_train[i]['review/text'].lower() if not c in punctuation])\n",
    "    text_list_m[i] = r.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_list_with_punc_m = defaultdict(list)\n",
    "for i in range(len(data_train)):\n",
    "    r = ''.join([c if not c in punctuation else ' '+c+' ' for c in data_train[i]['review/text'].lower()])\n",
    "    text_list_with_punc_m[i] = r.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we store data in memory-bigram\n",
    "# remove punctuation or not remove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [],
   "source": [
    "bi_text_list_m = defaultdict(list)\n",
    "for i in range(len(data_train)):\n",
    "    bi_unit = []\n",
    "    r = ''.join([c for c in data_train[i]['review/text'].lower() if not c in punctuation])\n",
    "    for j in range(len(r.split())-1):\n",
    "        bi_unit.append(r.split()[j]+\"-\"+r.split()[j+1])\n",
    "    bi_text_list_m[i] = bi_unit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [],
   "source": [
    "bi_text_list_with_punc_m = defaultdict(list)\n",
    "for i in range(len(data_train)):\n",
    "    bi_unit = []\n",
    "    r = ''.join([c if not c in punctuation else ' '+c+' ' for c in data_train[i]['review/text'].lower()])\n",
    "    for j in range(len(r.split())-1):\n",
    "        bi_unit.append(r.split()[j]+\"-\"+r.split()[j+1])\n",
    "    bi_text_list_with_punc_m[i] = bi_unit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word count \n",
    "# remove punctuation or not remove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordCount_m = defaultdict(int)\n",
    "for i in range(len(text_list_m)):\n",
    "    for w in text_list_m[i]:\n",
    "        wordCount_m[w] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordCount_with_punc_m = defaultdict(int)\n",
    "for i in range(len(text_list_with_punc_m)):\n",
    "    for w in text_list_with_punc_m[i]:\n",
    "        wordCount_with_punc_m[w] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bi_word count\n",
    "# remove punctuation or not remove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [],
   "source": [
    "bi_wordCount_m = defaultdict(int)\n",
    "for i in range(len(bi_text_list_m)):\n",
    "    for w in bi_text_list_m[i]:\n",
    "        bi_wordCount_m[w] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [],
   "source": [
    "bi_wordCount_with_punc_m = defaultdict(int)\n",
    "for i in range(len(bi_text_list_with_punc_m)):\n",
    "    for w in bi_text_list_with_punc_m[i]:\n",
    "        bi_wordCount_with_punc_m[w] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the top 1000 as features\n",
    "counts_m = [(wordCount_m[w], w) for w in wordCount_m]\n",
    "counts_m.sort()\n",
    "counts_m.reverse()\n",
    "words_m = [x[1] for x in counts_m[:1000]]\n",
    "wordId_m = dict(zip(words_m, range(len(words_m))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts_with_punc_m = [(wordCount_with_punc_m[w], w) for w in wordCount_with_punc_m]\n",
    "counts_with_punc_m.sort()\n",
    "counts_with_punc_m.reverse()\n",
    "words_with_punc_m = [x[1] for x in counts_with_punc_m[:1000]]\n",
    "wordsId_with_punc_m = dict(zip(words_with_punc_m, range(len(words_with_punc_m))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [],
   "source": [
    "bi_counts_m = [(bi_wordCount_m[biw], biw) for biw in bi_wordCount_m]\n",
    "bi_counts_m.sort()\n",
    "bi_counts_m.reverse()\n",
    "bi_words_m = [x[1] for x in bi_counts_m[:1000]]\n",
    "bi_wordId_m = dict(zip(bi_words_m, range(len(bi_words_m))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [],
   "source": [
    "bi_counts_with_punc_m = [(bi_wordCount_with_punc_m[biw], biw) for biw in bi_wordCount_with_punc_m]\n",
    "bi_counts_with_punc_m.sort()\n",
    "bi_counts_with_punc_m.reverse()\n",
    "bi_words_with_punc_m = [x[1] for x in bi_counts_with_punc_m[:1000]]\n",
    "bi_wordId_with_punc_m = dict(zip(bi_words_with_punc_m, range(len(bi_words_with_punc_m))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [],
   "source": [
    "each_freq_doc_m = defaultdict(int)\n",
    "for each_word in words_m:\n",
    "    freq = 0\n",
    "    for i in range(len(text_list_m)):\n",
    "        if each_word in text_list_m[i]:\n",
    "            freq += 1\n",
    "    each_freq_doc_m[each_word] = freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [],
   "source": [
    "each_freq_doc_with_punc_m = defaultdict(int)\n",
    "for each_word in words_with_punc_m:\n",
    "    freq = 0\n",
    "    for i in range(len(text_list_with_punc_m)):\n",
    "        if each_word in text_list_with_punc_m[i]:\n",
    "            freq += 1\n",
    "    each_freq_doc_with_punc_m[each_word] = freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [],
   "source": [
    "bi_each_freq_doc_m = defaultdict(int)\n",
    "for each_word in bi_words_m:\n",
    "    freq = 0\n",
    "    for i in range(len(bi_text_list_m)):\n",
    "        if each_word in bi_text_list_m[i]:\n",
    "            freq += 1\n",
    "    bi_each_freq_doc_m[each_word] = freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [],
   "source": [
    "bi_each_freq_doc_with_punc_m = defaultdict(int)\n",
    "for each_word in bi_words_with_punc_m:\n",
    "    freq = 0\n",
    "    for i in range(len(bi_text_list_with_punc_m)):\n",
    "        if each_word in bi_text_list_with_punc_m[i]:\n",
    "            freq += 1\n",
    "    bi_each_freq_doc_with_punc_m[each_word] = freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeTF_IDF(gword, content_list, each_freq, index):\n",
    "    unit_freq = each_freq[gword]\n",
    "    tf = 0\n",
    "    for elem in content_list[index]:\n",
    "        if elem == gword:\n",
    "            tf += 1\n",
    "    return tf*(np.log10(5000/(1+unit_freq)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfidf_feature(index, contents_list, each_freq, unitset, unitid):\n",
    "    feat = [0]*len(unitset)\n",
    "    goal_review = contents_list[index]\n",
    "    for w in goal_review:\n",
    "        if w in unitset and feat[unitid[w]] == 0:\n",
    "            feat[unitid[w]] = computeTF_IDF(w, contents_list, each_freq, index)\n",
    "    feat.append(1)\n",
    "    return feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [],
   "source": [
    "def freq_feature(index, contents_list, unitset, unitid):\n",
    "    feat = [0]*len(unitset)\n",
    "    goal_review = contents_list[index]\n",
    "    for w in goal_review:\n",
    "        if  w in unitset:\n",
    "            feat[unitid[w]] += 1\n",
    "    feat.append(1)\n",
    "    return feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_text_list_m = defaultdict(list)\n",
    "for i in range(len(data_validate)):\n",
    "    r = ''.join([c for c in data_validate[i]['review/text'].lower() if not c in punctuation])\n",
    "    p_text_list_m[i] = r.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_text_list_with_punc_m = defaultdict(list)\n",
    "for i in range(len(data_validate)):\n",
    "    r = ''.join([c if not c in punctuation else ' '+c+' ' for c in data_validate[i]['review/text'].lower()])\n",
    "    p_text_list_with_punc_m[i] = r.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_bi_text_list_m = defaultdict(list)\n",
    "for i in range(len(data_validate)):\n",
    "    bi_unit = []\n",
    "    r = ''.join([c for c in data_validate[i]['review/text'].lower() if not c in punctuation])\n",
    "    for j in range(len(r.split())-1):\n",
    "        bi_unit.append(r.split()[j]+\"-\"+r.split()[j+1])\n",
    "    p_bi_text_list_m[i] = bi_unit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_bi_text_list_with_punc_m = defaultdict(list)\n",
    "for i in range(len(data_validate)):\n",
    "    bi_unit = []\n",
    "    r = ''.join([c if not c in punctuation else ' '+c+' ' for c in data_validate[i]['review/text'].lower()])\n",
    "    for j in range(len(r.split())-1):\n",
    "        bi_unit.append(r.split()[j]+\"-\"+r.split()[j+1])\n",
    "    p_bi_text_list_with_punc_m[i] = bi_unit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_wordCount_m = defaultdict(int)\n",
    "for i in range(len(p_text_list_m)):\n",
    "    for w in p_text_list_m[i]:\n",
    "        p_wordCount_m[w] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_wordCount_with_punc_m = defaultdict(int)\n",
    "for i in range(len(p_text_list_with_punc_m)):\n",
    "    for w in p_text_list_with_punc_m[i]:\n",
    "        p_wordCount_with_punc_m[w] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_bi_wordCount_m = defaultdict(int)\n",
    "for i in range(len(p_bi_text_list_m)):\n",
    "    for w in p_bi_text_list_m[i]:\n",
    "        p_bi_wordCount_m[w] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_bi_wordCount_with_punc_m = defaultdict(int)\n",
    "for i in range(len(p_bi_text_list_with_punc_m)):\n",
    "    for w in p_bi_text_list_with_punc_m[i]:\n",
    "        p_bi_wordCount_with_punc_m[w] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_counts_m = [(p_wordCount_m[w], w) for w in p_wordCount_m]\n",
    "p_counts_m.sort()\n",
    "p_counts_m.reverse()\n",
    "p_words_m = [x[1] for x in p_counts_m[:1000]]\n",
    "p_wordId_m = dict(zip(p_words_m, range(len(p_words_m))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_counts_with_punc_m = [(p_wordCount_with_punc_m[w], w) for w in p_wordCount_with_punc_m]\n",
    "p_counts_with_punc_m.sort()\n",
    "p_counts_with_punc_m.reverse()\n",
    "p_words_with_punc_m = [x[1] for x in p_counts_with_punc_m[:1000]]\n",
    "p_wordsId_with_punc_m = dict(zip(p_words_with_punc_m, range(len(p_words_with_punc_m))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_bi_counts_m = [(p_bi_wordCount_m[biw], biw) for biw in p_bi_wordCount_m]\n",
    "p_bi_counts_m.sort()\n",
    "p_bi_counts_m.reverse()\n",
    "p_bi_words_m = [x[1] for x in p_bi_counts_m[:1000]]\n",
    "p_bi_wordId_m = dict(zip(p_bi_words_m, range(len(p_bi_words_m))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_bi_counts_with_punc_m = [(p_bi_wordCount_with_punc_m[biw], biw) for biw in p_bi_wordCount_with_punc_m]\n",
    "p_bi_counts_with_punc_m.sort()\n",
    "p_bi_counts_with_punc_m.reverse()\n",
    "p_bi_words_with_punc_m = [x[1] for x in p_bi_counts_with_punc_m[:1000]]\n",
    "p_bi_wordId_with_punc_m = dict(zip(p_bi_words_with_punc_m, range(len(p_bi_words_with_punc_m))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_each_freq_doc_m = defaultdict(int)\n",
    "for each_word in p_words_m:\n",
    "    freq = 0\n",
    "    for i in range(len(p_text_list_m)):\n",
    "        if each_word in p_text_list_m[i]:\n",
    "            freq += 1\n",
    "    p_each_freq_doc_m[each_word] = freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_each_freq_doc_with_punc_m = defaultdict(int)\n",
    "for each_word in p_words_with_punc_m:\n",
    "    freq = 0\n",
    "    for i in range(len(p_text_list_with_punc_m)):\n",
    "        if each_word in p_text_list_with_punc_m[i]:\n",
    "            freq += 1\n",
    "    p_each_freq_doc_with_punc_m[each_word] = freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_bi_each_freq_doc_m = defaultdict(int)\n",
    "for each_word in p_bi_words_m:\n",
    "    freq = 0\n",
    "    for i in range(len(p_bi_text_list_m)):\n",
    "        if each_word in p_bi_text_list_m[i]:\n",
    "            freq += 1\n",
    "    p_bi_each_freq_doc_m[each_word] = freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_bi_each_freq_doc_with_punc_m = defaultdict(int)\n",
    "for each_word in p_bi_words_with_punc_m:\n",
    "    freq = 0\n",
    "    for i in range(len(p_bi_text_list_with_punc_m)):\n",
    "        if each_word in p_bi_text_list_with_punc_m[i]:\n",
    "            freq += 1\n",
    "    p_bi_each_freq_doc_with_punc_m[each_word] = freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {},
   "outputs": [],
   "source": [
    "pX_1 = [freq_feature(index, p_text_list_m, p_words_m, p_wordId_m) for index in range(len(p_text_list_m))]\n",
    "pX_2 = [tfidf_feature(index, p_text_list_m, p_each_freq_doc_m, p_words_m, p_wordId_m) for index in range(len(p_text_list_m))]\n",
    "pX_3 = [freq_feature(index, p_text_list_with_punc_m, p_words_with_punc_m, p_wordsId_with_punc_m) for index in range(len(p_text_list_with_punc_m))]\n",
    "pX_4 = [tfidf_feature(index, p_text_list_with_punc_m, p_each_freq_doc_with_punc_m, p_words_with_punc_m, p_wordsId_with_punc_m) for index in range(len(p_text_list_with_punc_m))]\n",
    "pX_5 = [freq_feature(index, p_bi_text_list_m, p_bi_words_m, p_bi_wordId_m) for index in range(len(p_bi_text_list_m))]\n",
    "pX_6 = [tfidf_feature(index, p_bi_text_list_m, p_bi_each_freq_doc_m, p_bi_words_m, p_bi_wordId_m) for index in range(len(p_bi_text_list_m))]\n",
    "pX_7 = [freq_feature(index, p_bi_text_list_with_punc_m, p_bi_words_with_punc_m, p_bi_wordId_with_punc_m) for index in range(len(p_bi_text_list_with_punc_m))]\n",
    "pX_8 = [tfidf_feature(index, p_bi_text_list_with_punc_m, p_bi_each_freq_doc_with_punc_m, p_bi_words_with_punc_m, p_bi_wordId_with_punc_m) for index in range(len(p_bi_text_list_with_punc_m))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_prime_val =  [d['review/overall'] for d in data_validate]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = [d['review/overall'] for d in data_train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unigram + remove + freq\n",
    "X_1 = [freq_feature(index, text_list_m, words_m, wordId_m) for index in range(len(text_list_m))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unigram + remove + tfidf\n",
    "X_2 = [tfidf_feature(index, text_list_m, each_freq_doc_m, words_m, wordId_m) for index in range(len(text_list_m))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unigram + not remove + freq\n",
    "X_3 = [freq_feature(index, text_list_with_punc_m, words_with_punc_m, wordsId_with_punc_m) for index in range(len(text_list_with_punc_m))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unigram + not remove + tfidf\n",
    "X_4 = [tfidf_feature(index, text_list_with_punc_m, each_freq_doc_with_punc_m, words_with_punc_m, wordsId_with_punc_m) for index in range(len(text_list_with_punc_m))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bigram + remove + freq\n",
    "X_5 = [freq_feature(index, bi_text_list_m, bi_words_m, bi_wordId_m) for index in range(len(bi_text_list_m))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bigram + remove + tfidf\n",
    "X_6 = [tfidf_feature(index, bi_text_list_m, bi_each_freq_doc_m, bi_words_m, bi_wordId_m) for index in range(len(bi_text_list_m))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bigram + not remove + freq\n",
    "X_7 = [freq_feature(index, bi_text_list_with_punc_m, bi_words_with_punc_m, bi_wordId_with_punc_m) for index in range(len(bi_text_list_with_punc_m))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bigram + not remove + tfidf\n",
    "X_8 = [tfidf_feature(index, bi_text_list_with_punc_m, bi_each_freq_doc_with_punc_m, bi_words_with_punc_m, bi_wordId_with_punc_m) for index in range(len(bi_text_list_with_punc_m))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_out_MSE(x, y, x_p, y_p):\n",
    "    clf = linear_model.Ridge(1.0, fit_intercept=False)\n",
    "    clf.fit(x,y)\n",
    "    theta = clf.coef_\n",
    "    predictions = clf.predict(x_p)\n",
    "    MSE_temp = sum([(predictions[i]-y_p[i])**2 for i in range(len(y_p))])/len(y_p)\n",
    "    return MSE_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6717115308085453\n"
     ]
    }
   ],
   "source": [
    "# Unigram + remove + freq\n",
    "print (train_out_MSE(X_1, Y, pX_1, Y_prime_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6767064601852669\n"
     ]
    }
   ],
   "source": [
    "# Unigram + remove + tfidf\n",
    "print (train_out_MSE(X_2, Y, pX_2, Y_prime_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6392805857415549\n"
     ]
    }
   ],
   "source": [
    "# Unigram + not remove + freq\n",
    "print (train_out_MSE(X_3, Y, pX_3, Y_prime_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6450116486717069\n"
     ]
    }
   ],
   "source": [
    "# Unigram + not remove + tfidf\n",
    "print (train_out_MSE(X_4, Y, pX_4, Y_prime_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6155755380643481\n"
     ]
    }
   ],
   "source": [
    "# Bigram + remove + freq\n",
    "print (train_out_MSE(X_5, Y, pX_5, Y_prime_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6189277231579557\n"
     ]
    }
   ],
   "source": [
    "# Bigram + remove + tfidf\n",
    "print (train_out_MSE(X_6, Y, pX_6, Y_prime_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6531938827236892\n"
     ]
    }
   ],
   "source": [
    "# Bigram + not remove + freq\n",
    "print (train_out_MSE(X_7, Y, pX_7, Y_prime_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6588351631138297\n"
     ]
    }
   ],
   "source": [
    "# Bigram + not remove + tfidf\n",
    "print (train_out_MSE(X_8, Y, pX_8, Y_prime_val))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
